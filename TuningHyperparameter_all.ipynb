{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import   pandas  as  pd\n",
    "import   numpy   as  np\n",
    "import   seaborn as  sb\n",
    "import   matplotlib.pyplot as plt\n",
    "from  sklearn.metrics         import   accuracy_score\n",
    "from  sklearn.metrics         import   precision_score\n",
    "from  sklearn.metrics         import   recall_score\n",
    "from  sklearn.metrics         import   f1_score\n",
    "from  sklearn.metrics         import   matthews_corrcoef\n",
    "from  sklearn.metrics         import   classification_report  \n",
    "from  sklearn.model_selection import   train_test_split\n",
    "from  sklearn.model_selection import   GridSearchCV\n",
    "from  sklearn                 import   metrics\n",
    "from  statistics              import   mean\n",
    "from  sklearn.preprocessing   import   StandardScaler  \n",
    "from  sklearn.naive_bayes     import   GaussianNB\n",
    "from  sklearn.neighbors       import   KNeighborsClassifier\n",
    "from  sklearn.svm             import   SVC  \n",
    "from  sklearn.neural_network  import   MLPClassifier\n",
    "from  sklearn.tree            import   DecisionTreeClassifier\n",
    "from  sklearn.ensemble        import   RandomForestClassifier\n",
    "from  sklearn.ensemble        import   AdaBoostClassifier\n",
    "from  sklearn.linear_model    import   LogisticRegression\n",
    "\n",
    "####\n",
    "class  Tune_class():                        \n",
    "    def CreateDF_split(self, path):\n",
    "        dataset = pd.read_csv(path)\n",
    "        df  = pd.DataFrame(dataset) \n",
    "        dfd = df.drop(['uniprot'], axis=1)\n",
    "        X   = dfd.drop(['target'], axis=1)\n",
    "        y   = dfd['target'] \n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "        scaler = StandardScaler()  \n",
    "        scaler.fit(Xtrain)  \n",
    "        Xtrain = scaler.transform(Xtrain)  \n",
    "        Xtest  = scaler.transform(Xtest)  \n",
    "        return  Xtrain, Xtest, ytrain, ytest\n",
    "    ###\n",
    "    def SetParamtere(self):\n",
    "        nb = {\n",
    "               'var_smoothing': np.logspace(0, -9, num=100)\n",
    "        }\n",
    "        knn = {\n",
    "               'n_neighbors': np.arange(1, 16), \n",
    "               'metric'     : ['euclidean', 'manhattan', 'minkowski','mahalanobis'], \n",
    "               'weights'    : ['uniform', 'distance']  \n",
    "        }\n",
    "        svc = { \n",
    "               'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],          \n",
    "               'C'     : [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "       }\n",
    "        mlp = { \n",
    "               'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "               'solver'            : ['lbfgs', 'sgd', 'adam'],\n",
    "               'activation'        : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "               'learning_rate'     : ['constant', 'adaptive', 'invscaling'],\n",
    "               'alpha'             : [0.0001, 0.05]  \n",
    "        }\n",
    "        dt = { \n",
    "               'criterion'        : ['gini','entropy'],\n",
    "               'splitter'         : ['best', 'random'],\n",
    "               'max_depth'        : np.arange(2, 10),\n",
    "               'min_samples_split': np.arange(0, 10),\n",
    "               'min_samples_leaf' : np.arange(0, 10),\n",
    "               'max_features'     : np.arange(0, 15)\n",
    "        }\n",
    "        rf = { \n",
    "               'criterion'        : ['gini', 'entropy'],\n",
    "               'n_estimators'     : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "               'max_depth'        : [3, 10, 20],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf' : [1, 2, 4],\n",
    "               'max_features'     : ['auto', 'log2']\n",
    "        }\n",
    "        ada = { \n",
    "               'algorithm'        : ['SAMME', 'SAMME.R'],\n",
    "               'n_estimators'     : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]    \n",
    "        }\n",
    "        lr = {\n",
    "               'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "               'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "               'C'      : [0.01, 0.1, 1.0, 2.0]     \n",
    "        }\n",
    "        return  nb, knn, svc, mlp, dt, rf, ada, lr\n",
    "    ###\n",
    "    def Tune(self, model, params, Xtrain, ytrain):\n",
    "        GS = GridSearchCV(model, param_grid=params, cv=10, n_jobs=-1)  \n",
    "        GS.fit(Xtrain, ytrain)\n",
    "        return GS\n",
    "    \n",
    "    def ComputeMesure(self, gs, Xtest, ytest):\n",
    "        ypred = gs.predict(Xtest)\n",
    "        a = round(accuracy_score(ytest, ypred), 2)\n",
    "        p = round(mean(precision_score(ytest, ypred, average=None)), 2)\n",
    "        r = round(mean(recall_score(ytest, ypred, average=None)), 2)\n",
    "        f = round(mean(f1_score(ytest, ypred, average=None)), 2)\n",
    "        m = round(matthews_corrcoef(ytest, ypred), 2)       \n",
    "        return [a, p, r, f, m]\n",
    "    ###\n",
    "    def  main(self, path):\n",
    "        pnb, pknn, psvc, pmlp, pdt, prf, pada, plr = self.SetParamtere()\n",
    "        Xtrain, Xtest, ytrain, ytest = self.CreateDF_split(path)\n",
    "        \n",
    "        print('Best parameters for NB:')\n",
    "        gsNB = self.Tune(GaussianNB(), pnb, Xtrain, ytrain)\n",
    "        bpNB = gsNB.best_params_\n",
    "        print(bpNB)\n",
    "        #print(bpNB['var_smoothing'])\n",
    "        nb = self.ComputeMesure(gsNB, Xtest, ytest)\n",
    "        \n",
    "        print('Best parameters for KNN:')  \n",
    "        gsKNN = self.Tune(KNeighborsClassifier(), pknn, Xtrain, ytrain)\n",
    "        bpKNN = gsKNN.best_params_\n",
    "        print(bpKNN)\n",
    "        knn = self.ComputeMesure( gsKNN, Xtest, ytest)\n",
    "        \n",
    "        print('Best parameters for SVM:')\n",
    "        gsSVC = self.Tune(SVC(), psvc, Xtrain, ytrain)\n",
    "        print(gsSVC.best_params_)\n",
    "        svc = self.ComputeMesure(gsSVC, Xtest, ytest)\n",
    "        \n",
    "        print('Best parameters for MLP:')\n",
    "        gsMLP = self.Tune(MLPClassifier(), pmlp, Xtrain, ytrain)\n",
    "        print(gsMLP.best_params_)\n",
    "        mlp = self.ComputeMesure(gsMLP, Xtest, ytest)\n",
    "        \n",
    "        print('Best parameters for DT:')\n",
    "        gsDT = self.Tune(DecisionTreeClassifier(), pdt, Xtrain, ytrain)\n",
    "        print(gsDT.best_params_)\n",
    "        dt = self.ComputeMesure(gsDT, Xtest, ytest)                \n",
    "        \n",
    "        print('Best parameters for RF:')\n",
    "        gsRF = self.Tune(RandomForestClassifier(), prf, Xtrain, ytrain)\n",
    "        print(gsRF.best_params_)\n",
    "        rf = self.ComputeMesure(gsRF, Xtest, ytest)     \n",
    "        \n",
    "        print('Best parameters for ADA:')\n",
    "        gsADA = self.Tune(AdaBoostClassifier(), pada, Xtrain, ytrain)\n",
    "        print(gsADA.best_params_)\n",
    "        ada = self.ComputeMesure(gsADA, Xtest, ytest)  \n",
    "        \n",
    "        print('Best parameters for LR:')\n",
    "        gsLR = self.Tune(LogisticRegression(), plr, Xtrain, ytrain)\n",
    "        print(gsLR.best_params_)\n",
    "        lr = self.ComputeMesure(gsLR, Xtest, ytest)  \n",
    "        \n",
    "        acc = [nb[0], knn[0], svc[0], mlp[0], dt[0], rf[0], ada[0], lr[0]] \n",
    "        pre = [nb[1], knn[1], svc[1], mlp[1], dt[1], rf[1], ada[1], lr[1]] \n",
    "        rec = [nb[2], knn[2], svc[2], mlp[2], dt[2], rf[2], ada[2], lr[2]] \n",
    "        f1  = [nb[3], knn[3], svc[3], mlp[3], dt[3], rf[3], ada[3], lr[3]] \n",
    "        mcc = [nb[4], knn[4], svc[4], mlp[4], dt[4], rf[4], ada[4], lr[4]] \n",
    "        all = [acc, pre, rec, f1, mcc]\n",
    "        model = ['NB', 'KNN', 'SVM', 'MLP', 'DT', 'RF', 'ADA', 'LR']\n",
    "        mes = ['Accuracy', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "        df = pd.DataFrame(all, index=mes, columns=model)\n",
    "        return df\n",
    "    ###\n",
    "    def DrawHeapMap(self, df, path):\n",
    "        fig, ax = plt.subplots()    \n",
    "        hm = sb.heatmap(df, ax=ax, cmap=\"Greens\", xticklabels=1, vmin=0, vmax=1, linewidths=0.5, linecolor='black', annot=True, cbar_kws={'label':'measure', 'orientation':'vertical'}) \n",
    "        hm.set_yticklabels(hm.get_yticklabels()) \n",
    "        plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n",
    "        fig = hm.get_figure()\n",
    "        fig.savefig(path)\n",
    "\n",
    "###        \n",
    "obj_Tune = Tune_class()      \n",
    "DataFrame_Tune = obj_Tune.main('D:/GitHub-IBB/DATA/IF.csv')\n",
    "obj_Tune.DrawHeapMap(DataFrame_Tune, 'D:/GitHub-IBB/Result/IF_Tune.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
